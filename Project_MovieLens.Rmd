---
title: "MovieLens Rating Prediction Project"
author: "Nabeel Khan"
date: "21-May-2020"
output:
  pdf_document: 
    keep_tex: true
    number_sections: yes
    toc: yes
    highlight: tango
    df_print: kable
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
---

\section{Introduction} 
\label{sec:introduction}
This report is part of the ‘HarvardX: PH125.9x Data Science: Capstone’ course. In this report, we discuss various approaches to develop a recommender system using the acquired data science skills.


\subsection{Background} 
\label{sec:background}
The recommender systems predict the interests of users and recommend product items that are quite likely interesting for them\cite{rsystems,rsystems1}. The recommender systems use state of the art machine learning algorithms to make precise predictions. The recommender systems are commonly used by Netflix, YouTube, Spotify, Amazon, or Ebay.


One of the famous success stories of the recommender system is Netflix competition \cite{nfc}. In 2006, Netflix announced the awarding of one million dollar prize to a team, which can come up with the best filtering algorithm to predict user ratings for movies based on previous ratings. It took three years for a team to win the competition. The winning team improved the accuracy of the Netflix recommender system by 10%.
 

\subsection{Aim of Project} 
\label{sec:projectaim}
This project aims to develop a recommender system using a model to predict movie ratings based on suitable predictors. 

\section{Dataset and Evaluation Metric} 
\label{sec:datasetmetric}
Netflix dataset is not available online. So, we use the 10M version of MoiveLens dataset in this project \cite{dataset}. The dataset is generated by the GroupLens, which is a research lab in the Department of Computer Science and Engineering at the University of Minnesota. The dataset contains 10 million ratings. We will split the data into training and validation sets to develop our model. 


To evaluate the performance of the model,  we will use Root Mean Square Error (RMSE) \cite{rmse} as defined in Equation \ref{eq:rmse}. The RMSE is a standard method to measure the error of a model in predicting quantitative data.

\begin{equation}
\label{eq:rmse}
RMSE = \sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^{n} (\hat{y}_{i}-y_{i})^{2}}
\end{equation}

Where $\hat{y}_{i}$ refers to the predicted values by the model, ${y}_{i}$ refers to the actual values, and \emph{n} refers to the total number of observations. The RMSE is a commonly used metric for analyzing the performance of models, but it can produce biased
results in the presence of a large number of outliers or noise in the data. In this project, the RMSE will indicate how close model predictions are to the actual ratings in the validation set.

\subsection{Download Data}
\label{datadownload}

We download data from the website. Then, we split
data into a training set referred to as \emph{edx}, and test set
referred to as \emph{validation}. 10\% of the data is used for validation, and 90\% is used for training.

\begin{itemize}
\item The model is trained using \emph{edx} dataset, and 
we compute RMSE using \emph{validation} dataset.
\end{itemize}

```{r}
################################
#  Install packages (if not installed)
################################
# Note: this process could take a couple of minutes
repos_path<- "http://cran.us.r-project.org"
if(!require(tidyverse)) install.packages("tidyverse", repos =repos_path)
if(!require(caret)) install.packages("caret", repos = repos_path)
if(!require(data.table)) install.packages("data.table", repos =repos_path)
if(!require(lubridate)) install.packages("lubridate", repos = repos_path)
if(!require(dplyr)) install.packages("dplyr", repos = repos_path)
if(!require(sjmisc)) install.packages("dplyr", repos = repos_path)

################################
# Load libraries
################################
library(lubridate)
library(tidyverse)
library(dplyr)
library(lubridate)
library(sjmisc)
```

```{r}
################################
# Downloading data
################################
# MovieLens 10M dataset:
 # https://grouplens.org/datasets/movielens/10m/
 # http://files.grouplens.org/datasets/movielens/ml-10m.zip

url <- "http://files.grouplens.org/datasets/movielens/ml-10m.zip"
dl <- tempfile()
    download.file(url, dl)
  
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))), col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
   colnames(movies) <- c("movieId", "title", "genres")
   movies <- as.data.frame(movies) %>% mutate(movieId =   as.numeric(levels(movieId))[movieId], title = as.character(title), genres =    as.character(genres))

################################
# Creating edx and validation sets
################################
movielens <- left_join(ratings, movies, by = "movieId")


# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)

edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
 edx <- rbind(edx, removed)

 # Removing the objects from environment as no longer required
rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
\section{Data Exploration}
\label{sec:dataexploration}
In this section, we explore the dataset to get familiar with it. We also
perform some data wrangling. Data wrangling is the process of cleaning, structuring, or enriching raw data into the desired format to make better decisions and get meaningful insights.

The dataset contains six variables namely ``userID",
``movieID", ``rating", ``timestamp", ``title'", and ``genres". The ``timestamp" indicates the date on which a user recorded his reviews for a particular movie. Each row in the dataset represents a single rating of a user for a single movie.


```{r}
head(edx)
```
There are around nine million rows in \emph{edx} dataset, which is approximately 90% of the whole data, as mentioned before. The summary of \emph{edx} dataset shows that there are no null values in the training dataset.

```{r}
sprintf("Edx Dataset - Rows = %d  | Columns = %d",nrow(edx),ncol(edx))
```
```{r}
summary(edx)
```
\subsection{Data Wrangling}
\label{sec:dw}
We find a couple of issues with the \emph{edx} dataset :

\begin{enumerate}
\item The \emph{``title''} of each movie has its premiere year appended to it.
\item The \emph{``timestamp''} contains the number of seconds that have elapsed since January 1, 1970., which is hard to interpret.
\end{enumerate}

To resolve these issues, we perform the following operations on both training and validation datasets:

\begin{enumerate}
\item Create a new column \emph{``premiereYr''}, which will store the premier year of the movie. We remove the year information from \emph{``title''} feature.
\item Modify the \emph{``timestamp"} feature so that it contains the information in date format, which is easy to interpret. 
\end{enumerate}

We can see the impact of the above operations on our dataset by looking at the summary of the dataset. 


```{r}
#####################################
# EDX Dataset
#####################################
# Extracting premiere date from movie title
edx <-edx %>% extract(title, c("title", "premiereYr"), 
                      regex = "^(.*) \\(([0-9 ##\\-]*)\\)$")
# Converting to integer format from char
edx$premiereYr <- as.numeric(edx$premiereYr)

# Converting timestamp to date format
edx$timestamp <- as.Date(as_datetime((edx$timestamp), origin="1970-01-01"))

#####################################
# Validation Dataset
#####################################
# Extracting premiere date from movie title
validation <-validation %>% extract(title, c("title", "premiereYr"),
                                    regex = "^(.*) \\(([0-9 ##\\-]*)\\)$")
# Converting to integer format from char
validation$premiereYr <- as.numeric(validation$premiereYr)

# Converting timestamp to date format
validation$timestamp <- as.Date(as_datetime((validation$timestamp), origin="1970-01-01"))

head(edx)
```

\section{Data Analysis}
\label{sec:dataanalysis}
In the previous section, we understood the structure of the data and
performed a couple of data wrangling operations. In this section, we extract insights about all features of the \emph{edx} dataset. 

\subsection{Users}
\label{sec:users}

The dataset has around 69800 unique users.

```{r}
# Finding unique users
edx %>% summarize(users = n_distinct(userId))
```
Each user can rate more than one movie. On average, each user rates around 129 movies. If we look at the frequency of ratings for users, we find out that the distribution is skewed to the right. Most of the users rate a few movies, while some rate movies in thousands. To handle such variability, we can include a user penalty term in our model (if needed) to improve the performance. The process of adding a penalty term to error function is known as \textbf{Regularization} \cite{rafa}.

Regularization allows penalizing large estimates that are formed
using small sample sizes and avoid overfitting of data. Overfitting
occurs because the model tries too hard to capture the noise in the
training dataset. The basic idea of regularization is to constrain
the total variability of the effect sizes. 

```{r}
# Calculate the average number of ratings for users
edx %>% count(userId) %>% summarise(mean_rating=mean(n))
```

```{r}
# Plotting frequency of ratings per user 
edx %>%
  count(userId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30,  fill='cadetblue', color='black') +
  scale_x_log10() +
  xlab("Number of users") + 
  ylab("Fequency of ratings") +
  ggtitle("Distribution of ratings based on Users")
```
\subsection{Movies}
\label{sec:movies}
The dataset has around 10400 unique movies. 

```{r}
# Finding unique movies
edx %>% summarize(movies = n_distinct(title))
```
The release years for movies in \emph{edx} dataset range from 1915 to 2008. We can see an increase in the number of rated movies for the years 1980 to 1995. After 1995, the frequency of rated movies starts decreasing—one of the possible reasons is that the dataset has not included many movies for other years due to insufficient rating information or the number of users providing the rating information dropped.
```{r}
library(scales)
# Plotting frequency of rated moves over the years
# We use premiere years of movies as the range of years
edx %>%
  select(movieId, premiereYr) %>% # select columns we need
  group_by(premiereYr) %>% # group by year
  summarise(count = n())  %>% # count movies per year
  arrange(premiereYr)%>%
  ggplot(aes(x = premiereYr, y = count)) +
  scale_y_log10() +
  scale_y_continuous(breaks = c(seq(0, 800000, 100000)), labels=comma)+
  labs(x="Movie Premiere Years", y="Frequency of movies") +
  geom_point(color="cadetblue") +
  ggtitle("Distribution of rated movies with respect to premiere years")
 
```
Most movies get a decent number of ratings from the users. Although some movies have a less number of ratings. This indicates that movie information can also be used as penalty term in our model (if required).

```{r}
# Distribution or freqeuncy of ratings for all movies
edx %>% group_by(movieId) %>% summarize(n = n()) %>%
  ggplot(aes(n)) + geom_histogram(bins=30, fill = 'cadetblue', color = 'black') +
  scale_x_log10() +
  xlab("Number of movies") + 
  ylab("Fequency of ratings") +
  ggtitle("Distribution of ratings per movie")
```


\subsection{Ratings}
\label{sec:ratings}
As we discussed in Section \label{sec:movies} that the release years of movies in the dataset range from 1915 to 2008. The dataset also indicates when users recorded the ratings for each movie. A close look suggests that rating data covers a reasonable period from years 1995 to 2009 i.e., 14 years.

```{r}
# Analysing when ratings data was added by users
years<-format(edx$timestamp, format="%Y")
sprintf("Rating year (minimum) =%s", min(years))
sprintf("Rating year (maximu) =%s", max(years))
```
The movies can have either half-star or full-stars ratings. The distribution indicates that most movies get full star ratings. We observe that there is a tendency that users generally rate movies 3 or 4.

```{r}

# Generating groups of data for half star and full star ratings
star_ratings_group <-  ifelse((edx$rating == 1 |edx$rating == 2 | edx$rating == 3 | 
                  edx$rating == 4 | edx$rating == 5) ,
                   "Full_star", 
                   "Half_star") 

# Plotting the distribution of ratings in terms of half and full star
ratings_distribution <- data.frame(edx$rating, star_ratings_group)
ggplot(ratings_distribution, aes(x= edx.rating, fill = star_ratings_group)) +
  geom_histogram( binwidth = 0.2,color = 'black') +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000)))+
  labs(x="Ratings", y="Number of ratings") +
  ggtitle("Distribution of ratings")
```

We now explore the top 10 and bottom 10 movies based on the total number of ratings.  Top 10 movies are well-known and many users rate them as expected. Contrary, the bottom 10 movies appear to be obscure, and only a few users rate them. Therefore, the predictions of future ratings for such movies will be difficult. There are 125 movies in the dataset with a single user rating. This information is critical because a low rating numbers can result in overfitting for our model. 

```{r}
movies_count<-(edx %>% group_by(title) %>%summarize(count=n()))
sprintf("Movies with a single rating = %d",sum(movies_count$count==1))
```

```{r}
top_movies <- edx %>%
  group_by(title) %>%
  summarize(count=n()) %>%
  top_n(10) %>%
  arrange(desc(count))

top_movies %>% 
  ggplot(aes(x=reorder(title, count), y=count)) +
  geom_bar(stat='identity', fill="cadetblue") + coord_flip(y=c(0, 40000)) +
  labs(x="", y="Number of ratings") +
  geom_text(aes(label= count), hjust=-0.1, size=3) +
  labs(title="Top 10 movies titles based on number of ratings" )
```

```{r}
bottom_movies <- edx %>%
  group_by(title) %>%
  summarize(count=n()) %>%
  arrange(desc(count))

bottom_movies %>% tail(10) %>%
  ggplot(aes(x=reorder(title, count), y=count)) +
  geom_bar(stat='identity', fill="cadetblue") + coord_flip(y=c(0, 100)) +
  labs(x="", y="Number of ratings") +
  geom_text(aes(label= count), hjust=-0.1, size=3) +
  labs(title="Bottom 10 movies based on number of ratings")
```
\subsection{Generes}
\label{sec:generes}
Each movie can have more than one genre. A close analysis indicates that most movies in the dataset belong to ``Comedy" and ``Drama" genres. A very few movies belong to the ``fantasy" genre. Therefore, we can also use this feature as a penalty term in our model (if required). Moreover, there are some movies with untitled genres. We can remove them if needed. 

```{r}
# Extracting all unique combination of genres
unique_genres_combination<-edx%>% group_by(genres)%>%summarize(count = n())

# Extract all possible genres (which exist in the dataset)
movie_genres <- unique(unlist(strsplit(unique(edx$genres), split='|', fixed=TRUE)))
movie_genres <- as.data.frame(movie_genres)
movie_genres<-movie_genres %>% add_column(count = 0)

# Looping through to find the freqeucny or distribution of each genre
for(i in 1:dim(movie_genres)[1]) {
  for(j in 1:dim(unique_genres_combination)[1])
  {
    match = as.character(unique_genres_combination[j,1])
    count = as.numeric(unique_genres_combination[j,2])
    # See if genre category exists in the combination
    if (str_contains(movie_genres[i,1],match))
    {
      movie_genres[i,2]<-movie_genres[i,2]+count
    }
  }
}

# Plot distrobution of movie genres 
movie_genres%>%
  ggplot(aes(x=reorder(movie_genres, count), y=count)) +
  geom_bar(stat='identity', fill="cadetblue") + coord_flip(y=c(0, 900000)) +
  labs(x="Count", y="Movie genres") +
  geom_text(aes(label= count), hjust=-0.1, size=3) +
  labs(title="Distribution of movies based on genre" )
```

\subsection{Premier Year}
\label{sec:pyear}

We analyse the rating trends of users over the years. Interestingly, we find out that mean ratings have dropped over the years. This indicates that users are not rating more movies in recent years.
```{r}
edx %>% group_by(premiereYr) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(premiereYr, rating)) +
  geom_point() +
  geom_smooth(alpha=0.5, color='cadetblue',method = lm,formula = y ~ splines::bs(x, 3))
```\section{Methods}
\label{sec:methods}

We will use RMSE to estimate the quality of our model. Using equation \ref{eq:rmse}, we write the following function to compute the RMSE between actual ratings and predicted ratings:: 

```{r}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The lower RMSE is better.The evaluation criteria for this project is that RMSE of the proposed model should be less than 0.8649.

```{r}
#Initiate RMSE results to compare various models
rmse_results <- data_frame(method = "Target RSME", RMSE = 0.8649)
rmse_results
```

\subsection{Average Movie Rating Model}
\label{sec:am}
We start with the simplest model by predicting the same ratings for all movies regardless of users or freqeuncy of ratings. In other words, we compute the dataset's mean raitng as follows: 

\begin{equation}
Y_{u, i} = \mu + \epsilon_{u, i}
\end{equation}


Where $\epsilon_{u,i}$ is the independent error sample from the same distribution centered at 0 and $\mu$ the ``true" rating for all movies. This model is based on the assumption that differences in movie ratings can be explained by random variation alone. The expected value of the data is around 3.5 and we obtain a RMSE of 1.06.
```{r}
# Calculating mean of ratings
mu <- mean(edx$rating)
mu

```
We make predictions using $\mu$ and obtain our firt RMSE score of 1.06.
```{r}
# Making predictions using mean rating
naive_rmse <- RMSE(validation$rating, mu)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Average Rating Model",  
                                     RMSE = naive_rmse ))
rmse_results

```

In order to improve the RMSE of our model, we have to include extra parameters in our model. We can use some of the insights obtained in previous section, such as user effect.

\subsection{Movie Effect Model}
\label{sec:mem}
We recall from Section \ref{sec:dataanalsyis} that different movies are rated differently. The popular movies are rated much higher than unpopular movies. 
We can modify our previous model by adding the term `$b_{i}$" to represent average ranking for movie i as follows:

\begin{equation}
Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}
\end{equation}


The least squares estimate $b_{i}$ is just the average of $Y_{u,i} - \mu$ for each movie $i$. So we can compute it as follows: 

```{r}
mu <- mean(edx$rating)
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
```

The histogram is skewed to the left, which implies that most movies have negative effects. This is known as penalty term movie effect. 

```{r}
movie_avgs%>%
ggplot(aes(b_i)) +
geom_histogram(bins = 30, color = "black")
```
If one movie is on average rated worse than the average rating of all movies $\mu$, then we predict that it will rated lower that $\mu$ by $b_{i}$.By including the movie effect, our model's RMSE improves to 0.9437 on validation data. 

```{r}
predicted_ratings <- mu +  validation %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect model",  
                                     RMSE = model_1_rmse ))
rmse_results
```

\section{User Effects}
\label{sec:ue}
We know that some users are more active than others.We compute the average rating for user $\mu$, for those that have rated over 50 movies. There is also substantial variability acorss users. Some users are very cranky (like few movies), and other users love every movie. Therefore, we can further improve our previous model by adding user-specific effect ($b_{u}$): 

\begin{equation}
Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}
\end{equation}

```{r}
edx %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating - mu - b_i)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 10, color = "black")
```

If a cranky user (negative $b_{u}$) rates a great movie (positive $b_{i}$), the effects counter each other. And, we may be able to correctly predict that this user gave this great movie a 3 rather than a 5. We estimate  $b_{u}$ as the average of $$Y_{u, i} - \mu - b_{i}$$ as shown below.

```{r}
user_avgs <- edx %>%
left_join(movie_avgs, by='movieId') %>%
group_by(userId) %>%
summarize(b_u = mean(rating - mu - b_i))
```

By using new model for prediction, the RMSE improves to 0.865.

```{r}
predicted_ratings <- validation%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and user effect model",  
                                     RMSE = model_2_rmse))
rmse_results
```
\subsection{Regularization Using Moives and Users}
\subsection{sec:reg}
There is still room for improvement as we stil made mistakes on our first model (using only movies). There are users who have rated very few movies (less than 30 movies). On the other hand, some movies are rated very few times (say 1 or 2). These are basically noisy estimates that result in overfitting of the model. Such large errors are likely increase the RMSE and affect our predictions. 

To tackle this problem, we use the concept of regularization. The regularization permits to penalize large estimates that come from small sample sizes. First, we find the value of lambda (that is a tuning parameter) that will minimize the RMSE.


```{r}
library(tidyverse)
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(edx$rating)
  
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, validation$rating))
})

```

We can see that the optimal lambda value is 5.25, which gives us a RMSE of 0.8649.

```{r plot_lambdas, echo = TRUE}
qplot(lambdas, rmses)  
lambda <- lambdas[which.min(rmses)]
lambda
```


```{r rmse_results2, echo = TRUE}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized movie and user effect model",  
                                     RMSE = min(rmses)))
rmse_results
```
\section{Results}
\label{sec:results}
We can analyse the RMSEs for the various models trained in the previouse section. We started with a very basic approach of predicting the mean ratingregardless of the user. Then, we added movie and user effects to the simple model. Finally, we experimented  regularization with movie and user effects. For regularization, we found the optimal value of turing parameter lambda.The resultant RMSEs are as follows:

```{r}
rmse_results
```

We can see that the RMSE of our final model is 0.864817.  This is a significant improvement (decrease in RMSE of around 18.5%) compared to the first simple model. 

\section{Conclusion}
\label{sec:conclusion}
The simplest model which predicts the same rating (mean rating) for all movies regardless of user gave anRMSE above 1. If RMSE is larger than 1, it means our typical error is larger than one star, which is notgood. By taking into consideration the movie effect the RMSE went down to 0.9439087 which was a greatimprovement. RMSE further went down to 0.8653488 after modelling the user effect in the previous model.However, the lowest RMSE i.e, 0.8648201 was achieved by regularizing the movie and user effect, whichpenalized larger estimates of ratings from relatively small sample size of users.Subsequently, regularizing the year and genres effects can further improve the residual mean squared errorbut regularizing the genres is computationally very expensive since it requires separating multiple genres formany movies into multiple observations for a given movie i having single genre in genres column.

In this report we described a way to build up the recommendation algorithm to predict movie ratings using Movielens data set step by step. Four predictors were used in our algorithm included: (i) the baseline as average rating of each movie, (ii) the specific-effect by user, (iii) the specific-effect by aging time, (iv) the specific-effect by genres. Two main predictors have highest impact to the results are the average rating of each movie and the specific-effect by user. The final RMSE from our algorithm is 0.8645. This result achieved our project goals and the initial criteria of the course HarvardX - PH125.9X Data Science: Capstone project - All learners (RMSE < 0.8649).Because our algorithm is based on the average rating of each movie, therefore if a movie have very less number of rating, this algorithm is limited to provide an accuracy results. A better result is received only when the number of rating of a movie and/or the number of rating given by a user is increased high enough.However this is an opportunity to improve our algorithm in the future by including the number of rating of a movie and the number of rating given by a user to our algorithm. Furthermore, other machine learning techniques such as Regularization, Penalized Least Squares, Matrix Factorization could also improve the results further.Another limitation during this project time is the hardware, especially RAM as the main constraint, and the speed of normal laptop.
```{r}
rmse_results
```



\begin{thebibliography}{9}
\bibitem{rsystems} 
Baptiste Rocca -Introduction to recommender systems \url{https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada}
\bibitem{rsystems1}
Resnick, P., Varian, H. \textit{Recommender Systems}. CACM 40(3), 56–58 (1997)
\bibitem{nfc}
Netflix Prize \url{https://en.wikipedia.org/wiki/Netflix_Prize}

\bibitem{dataset}
MovieLens 10M Dataset \url{https://grouplens.org/datasets/movielens/10m}

\bibitem{rmse}
James Moody - What does RMSE really mean? \url{https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e}

\bibitem{rafa}
Rafael Irizarry - Introduction to Data Science \url{https://rafalab.github.io/dsbook}


\end{thebibliography}
